@online{ArticleCitationsReferences,
  title = {Article {{Citations}} - {{References}} - {{Scientific Research Publishing}}},
  url = {https://www.scirp.org/reference/referencespapers},
  urldate = {2025-12-16},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-12-16T20:56:21.705Z},
  file = {/Users/gat/Zotero/storage/GB5N7N33/referencespapers.html}
}

@online{ComprehensiveListLarge,
  title = {A Comprehensive List of {{Large Language Model}} Knowledge Cut off Dates - {{ALLMO}}: {{Boost Your Brand}}’s {{Visibility}} in {{AI Search}}},
  shorttitle = {A Comprehensive List of {{Large Language Model}} Knowledge Cut off Dates - {{ALLMO}}},
  url = {https://www.allmo.ai/articles/list-of-large-language-model-cut-off-dates},
  urldate = {2025-11-12},
  abstract = {Knowledge cut-off dates play a critical role in optimizing content for LLMs. This article offers a comprehensive and regularly updated overview of the cut-off dates for major models - including ChatGPT, Gemini, Claude, and others. Along with practical guidance on how to account for these timelines when optimizing for LLM visibility.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-12T19:07:27.080Z},
  file = {/Users/gat/Zotero/storage/7GV56VSP/list-of-large-language-model-cut-off-dates.html}
}

@article{devijverBaumsForwardbackwardAlgorithm1985,
  title = {Baum's Forward-Backward Algorithm Revisited},
  author = {Devijver, Pierre A},
  date = {1985-12-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {3},
  number = {6},
  pages = {369--373},
  issn = {0167-8655},
  doi = {10.1016/0167-8655(85)90023-6},
  url = {https://www.sciencedirect.com/science/article/pii/0167865585900236},
  urldate = {2025-12-16},
  abstract = {In this note, we examine the forward-backward algorithm from the computational viewpoint of the underflow problem inherent in Baum's (1972) original formulation. We demonstrate that the conversion of Baum's computation of joint likelihoods into the computation of posterior probabilities results in essentially the same algorithm, except for the presence of a scaling factor suggested by Levinson et al. (1983) on rather heuristic grounds. The resulting algorithm is immune to the underflow problem, and Levinson's scaling method is given a theoretical justification. We also investigate the relationship between Baum's algorithm and the recent algorithms of Askar and Derin (1981) and Devijver (1984).},
  keywords = {forward-backward algorithm,Hidden Markov chains,maximum a posteriori probability estimation,maximum likelihood estimation},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-12-16T20:57:40.214Z},
  file = {/Users/gat/Zotero/storage/NMN5JQKS/0167865585900236.html}
}

@article{friedmanValuePrecisionProbability2018,
  title = {The {{Value}} of {{Precision}} in {{Probability Assessment}}: {{Evidence}} from a {{Large-Scale Geopolitical Forecasting Tournament}}},
  shorttitle = {The {{Value}} of {{Precision}} in {{Probability Assessment}}},
  author = {Friedman, Jeffrey A. and Baker, Joshua D. and Mellers, Barbara A. and Tetlock, Philip E. and Zeckhauser, Richard},
  date = {2018-06-01},
  journaltitle = {International Studies Quarterly},
  shortjournal = {Int Stud Q},
  volume = {62},
  number = {2},
  pages = {410--422},
  publisher = {Oxford Academic},
  issn = {0020-8833},
  doi = {10.1093/isq/sqx078},
  url = {https://dx.doi.org/10.1093/isq/sqx078},
  urldate = {2025-11-05},
  abstract = {Abstract. Scholars, practitioners, and pundits often leave their assessments of uncertainty vague when debating foreign policy, arguing that clearer probab},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-12-16T16:08:32.170Z},
  file = {/Users/gat/Zotero/storage/BLGB9LIY/Friedman et al. - 2018 - The Value of Precision in Probability Assessment Evidence from a Large-Scale Geopolitical Forecasti.pdf}
}

@online{GenLM,
  title = {{{GenLM}}},
  url = {https://genlm.org/},
  urldate = {2025-11-12},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-12T22:43:05.932Z},
  file = {/Users/gat/Zotero/storage/6TNUBVFM/genlm.org.html}
}

@online{GenlmGenlmcontrolControlled,
  title = {Genlm/Genlm-Control: {{Controlled}} Text Generation with Programmable Constraints},
  url = {https://github.com/genlm/genlm-control},
  urldate = {2025-11-12},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-12T22:40:00.206Z},
  file = {/Users/gat/Zotero/storage/MCWX7T8T/genlm-control.html}
}

@online{jinTimeSeriesForecasting2024,
  title = {Time {{Series Forecasting}} with {{LLMs}}: {{Understanding}} and {{Enhancing Model Capabilities}}},
  shorttitle = {Time {{Series Forecasting}} with {{LLMs}}},
  author = {Jin, Mingyu and Tang, Hua and Zhang, Chong and Yu, Qinkai and Liu, Chengzhi and Zhu, Suiyuan and Zhang, Yongfeng and Du, Mengnan},
  date = {2024-02-16},
  eprint = {2402.10835},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.10835},
  url = {http://arxiv.org/abs/2402.10835},
  urldate = {2025-10-30},
  abstract = {Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computation and Language},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-10-30T18:13:50.535Z},
  file = {/Users/gat/Zotero/storage/32HJFCRY/Jin et al. - 2024 - Time Series Forecasting with LLMs Understanding and Enhancing Model Capabilities.pdf;/Users/gat/Zotero/storage/WCCFXSZ2/2402.html}
}

@inproceedings{liederBurninBiasRationality2012,
  title = {Burn-in, Bias, and the Rationality of Anchoring},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lieder, Falk and Griffiths, Tom and Goodman, Noah},
  date = {2012},
  volume = {25},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2012/hash/81e5f81db77c596492e6f1a5a792ed53-Abstract.html},
  urldate = {2025-11-05},
  abstract = {Bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind. Unfortunately, exact Bayesian inference is intractable in all but the simplest models. Therefore minds and machines have to approximate Bayesian inference. Approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff? We investigate time-accuracy tradeoffs using the Metropolis-Hastings algorithm as a metaphor for the mind's inference algorithm(s). We find that reasonably accurate decisions are possible long before the Markov chain has converged to the posterior distribution, i.e. during the period known as burn-in. Therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value. The resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic. The model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks. Our theoretical and empirical results suggest that the anchoring bias is consistent with approximate Bayesian inference.},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-11-05T04:51:05.965Z},
  file = {/Users/gat/Zotero/storage/6YHNNXRV/Lieder et al. - 2012 - Burn-in, bias, and the rationality of anchoring.pdf}
}

@online{lipkinFastControlledGeneration2025,
  title = {Fast {{Controlled Generation}} from {{Language Models}} with {{Adaptive Weighted Rejection Sampling}}},
  author = {Lipkin, Benjamin and LeBrun, Benjamin and Vigly, Jacob Hoover and Loula, João and MacIver, David R. and Du, Li and Eisner, Jason and Cotterell, Ryan and Mansinghka, Vikash and O'Donnell, Timothy J. and Lew, Alexander K. and Vieira, Tim},
  date = {2025-08-19},
  eprint = {2504.05410},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.05410},
  url = {http://arxiv.org/abs/2504.05410},
  urldate = {2025-11-13},
  abstract = {The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed \$100,000\$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-13T01:20:18.276Z},
  file = {/Users/gat/Zotero/storage/SQS3XDHP/Lipkin et al. - 2025 - Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling.pdf}
}

@online{loulaSyntacticSemanticControl2025,
  title = {Syntactic and {{Semantic Control}} of {{Large Language Models}} via {{Sequential Monte Carlo}}},
  author = {Loula, João and LeBrun, Benjamin and Du, Li and Lipkin, Ben and Pasti, Clemente and Grand, Gabriel and Liu, Tianyu and Emara, Yahya and Freedman, Marjorie and Eisner, Jason and Cotterell, Ryan and Mansinghka, Vikash and Lew, Alexander K. and Vieira, Tim and O'Donnell, Timothy J.},
  date = {2025-04-18},
  eprint = {2504.13139},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.13139},
  url = {http://arxiv.org/abs/2504.13139},
  urldate = {2025-10-25},
  abstract = {A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as probabilistic conditioning, but exact generation from the resulting distribution -- which can differ substantially from the LM's base distribution -- is generally intractable. In this work, we develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains -- Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis -- we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8x larger, as well as closed-source, fine-tuned ones. In support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. Our system builds on the framework of Lew et al. (2023) and integrates with its language model probabilistic programming language, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-10-25T20:23:21.836Z},
  file = {/Users/gat/Zotero/storage/J5YEILZE/Loula et al. - 2025 - Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo.pdf;/Users/gat/Zotero/storage/FFGLB9GG/2504.html}
}

@software{madjeTypst2025,
  title = {Typst},
  author = {Mädje, Laurenz and Haug, Martin and {The Typst Project Developers}},
  date = {2025-11-12T18:19:54Z},
  origdate = {2019-09-24T21:41:56Z},
  url = {https://github.com/typst/typst},
  urldate = {2025-11-12},
  abstract = {A markup-based typesetting system that is powerful and easy to learn.},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-12T18:26:15.899Z}
}

@online{MetallamaLlama321BHugging2024,
  title = {Meta-Llama/{{Llama-3}}.2-{{1B}} · {{Hugging Face}}},
  date = {2024-12-06},
  url = {https://huggingface.co/meta-llama/Llama-3.2-1B},
  urldate = {2025-11-13},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-13T00:44:38.957Z},
  file = {/Users/gat/Zotero/storage/UA2C7C2H/Llama-3.html}
}

@book{tetlockSuperforecastingArtScience2015,
  title = {Superforecasting: The Art and Science of Prediction},
  shorttitle = {Superforecasting},
  author = {Tetlock, Philip E. and Gardner, Dan},
  date = {2015},
  edition = {First edition},
  publisher = {Crown Publishers},
  location = {New York},
  abstract = {"From one of the world's most highly regarded social scientists, a transformative book on the habits of mind that lead to the best predictions Everyone would benefit from seeing further into the future, whether buying stocks, crafting policy, launching a new product, or simply planning the week's meals. Unfortunately, people tend to be terrible forecasters. As Wharton professor Philip Tetlock showed in a landmark 2005 study, even experts' predictions are only slightly better than chance. However, an important and underreported conclusion of that study was that some experts do have real foresight, and Tetlock has spent the past decade trying to figure out why. What makes some people so good? And can this talent be taught? In Superforecasting, Tetlock and coauthor Dan Gardner offer a masterwork on prediction, drawing on decades of research and the results of a massive, government-funded forecasting tournament. The Good Judgment Project involves tens of thousands of ordinary people--including a Brooklyn filmmaker, a retired pipe installer, and a former ballroom dancer--who set out to forecast global events. Some of the volunteers have turned out to be astonishingly good. They've beaten other benchmarks, competitors, and prediction markets. They've even beaten the collective judgment of intelligence analysts with access to classified information. They are "superforecasters." In this groundbreaking and accessible book, Tetlock and Gardner show us how we can learn from this elite group. Weaving together stories of forecasting successes (the raid on Osama bin Laden's compound) and failures (the Bay of Pigs) and interviews with a range of high-level decision makers, from David Petraeus to Robert Rubin, they show that good forecasting doesn't require powerful computers or arcane methods. It involves gathering evidence from a variety of sources, thinking probabilistically, working in teams, keeping score, and being willing to admit error and change course. Superforecasting offers the first demonstrably effective way to improve our ability to predict the future--whether in business, finance, politics, international affairs, or daily life--and is destined to become a modern classic"--},
  isbn = {978-0-8041-3669-3},
  pagetotal = {340},
  keywords = {BUSINESS & ECONOMICS / Forecasting,Economic forecasting,Forecasting,PSYCHOLOGY / Cognitive Psychology,SOCIAL SCIENCE / Future Studies},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-11-05T04:03:46.972Z}
}
